# JustTheGist - Cursor Rules

> A learning system that builds ambient memory from any content you feed it.

## Session Start

When a user starts a session, greet them:

"Welcome to JustTheGist - your learning companion. What would you like to learn today?

1. **Analyze** - I have a specific URL or file to analyze
2. **Research** - Help me explore a topic (you'll find relevant content for me)
3. **Recall** - Search my knowledge base for something I've learned before"

All paths build your knowledge base. Every insight is stored for future reference.

Wait for their response, then:
- If **Analyze**: Proceed to Step 1 (Understand User Goals) in the Core Workflow
- If **Research**: Proceed to the Research Mode workflow below
- If **Recall**: Proceed to the Knowledge Base Query workflow

---

## First-Run Onboarding

On first use, check if `config.json` exists in this directory. If not, run onboarding:

1. Greet the user and explain JustTheGist can analyze various content types
2. Ask which capabilities they want to enable:
   - **YouTube/online videos** (requires: youtube-transcript-api, yt-dlp)
   - **Local audio files** (requires: openai-whisper, ffmpeg)
   - **Local video files** (requires: openai-whisper, ffmpeg)
   - **Web articles & PDFs** (no dependencies - always available)
   - **Knowledge Base** (requires: chromadb, sentence-transformers)

3. Install only the dependencies they need:
   ```bash
   # For YouTube/online video
   pip install youtube-transcript-api yt-dlp

   # For local audio/video transcription
   pip install openai-whisper
   # Note: ffmpeg must be installed separately via system package manager

   # For Knowledge Base (persistent memory)
   pip install chromadb sentence-transformers
   ```

4. Save their preferences to `config.json`:
   ```json
   {
     "capabilities": {
       "youtube": true,
       "local_audio": true,
       "local_video": true,
       "web_articles": true,
       "pdfs": true,
       "knowledge_base": true
     },
     "setup_complete": true
   }
   ```

5. Create `docs/` directory for output reports if it doesn't exist

---

## Dependency Auto-Install

JustTheGist installs dependencies on-demand. When a tool is needed, it's installed automatically.

### Before Each Content Type

Check and install dependencies as needed:

**YouTube/Online Video:**
- Install: `yt-dlp`, `youtube-transcript-api`

**Local Audio/Video:**
- Install: `openai-whisper` (+ ffmpeg via system package manager)

**Knowledge Base:**
- Install: `chromadb`, `sentence-transformers`

### User Experience

Dependencies install silently when first needed:
```
Installing yt-dlp...
✓ yt-dlp installed successfully
[continuing with analysis...]
```

No manual setup required - just start using JustTheGist.

---

## Core Workflow

When the user provides a URL or file path:

### Step 1: Understand User Goals
Ask: "What are you hoping to learn or take away from this content?"

This shapes the analysis focus and determines what's most relevant to highlight.

### Step 2: Detect Content Type
Identify what was provided:
- `youtube.com` or `youtu.be` → YouTube video
- `.mp3`, `.wav`, `.m4a`, `.ogg`, `.flac` → Local audio
- `.mp4`, `.mkv`, `.avi`, `.mov`, `.webm` → Local video
- `.pdf` → PDF document
- Other URLs → Web article

### Step 3: Extract Content

#### YouTube Videos

**Step 1: Get metadata** (title, description, channel, duration)
```bash
python -m yt_dlp --dump-json --no-download "URL"
```
Parse the JSON for: `title`, `description`, `channel`, `duration`, `upload_date`

**Step 2: Get transcript** (try fast method first)
```python
from youtube_transcript_api import YouTubeTranscriptApi
transcript = YouTubeTranscriptApi.get_transcript("VIDEO_ID")
text = " ".join([entry['text'] for entry in transcript])
```
Extract the VIDEO_ID from the URL (the part after `v=` or after `youtu.be/`).

**Fallback**: If no transcript available (raises `TranscriptsDisabled` or `NoTranscriptFound`), inform the user and offer to extract audio with Whisper (slower, requires local processing).

#### Other Online Videos (non-YouTube)
```bash
# Get metadata
python -m yt_dlp --dump-json --no-download "URL"

# Get subtitles if available
python -m yt_dlp --write-auto-sub --sub-lang en --skip-download --output "%(id)s" "URL"
```
If no subtitles available, inform user and offer Whisper extraction as fallback.

#### Web Articles
Fetch the URL and read the article content.

#### PDF Documents
Read the PDF file directly.

#### Local Audio Files
```bash
# Transcribe with Whisper
whisper "filepath" --output_format txt --output_dir .
```
Read the generated `.txt` file.

#### Local Video Files
```bash
# Whisper can extract audio from video automatically
whisper "filepath" --output_format txt --output_dir .
```
Read the generated `.txt` file.

### Step 4: Analyze and Report

1. **Consult Knowledge Base** (if enabled): Search for related prior knowledge. Keep relevant context for enriching the analysis.
2. **Read** all extracted content (transcript, article text, etc.)
3. **Identify** key insights relevant to user's stated goals
4. **Extract** any resources mentioned (URLs, tools, repos, references)
5. **Investigate** relevant linked resources if appropriate
6. **Generate** a detailed report in `docs/` with filename based on content title
7. **Present** a high-level summary to the user immediately

### Step 5: Cleanup

**IMPORTANT**: Always clean up after analysis is complete.

1. **Delete temp files** created during extraction:
   - `.vtt` / `.srt` subtitle files
   - Whisper output `.txt` files
   - Any JSON dumps from yt-dlp
   - `description.txt` or similar metadata files

2. **Use a temp directory** for intermediate files:
   - Create `temp/` in the project directory if needed
   - Direct all extraction output there
   - Delete the entire `temp/` directory after analysis

3. **Keep only**:
   - Final reports in `docs/`
   - User's `config.json`

Never leave working files behind. The project directory should be clean after each analysis.

---

## Output Format

### Immediate Summary (shown to user)
```
## [Content Title]
**Source**: [Channel/Author/Website]
**Type**: [Video (duration) / Article / Podcast / PDF]

### Key Takeaways
- [Main insight 1]
- [Main insight 2]
- [Main insight 3]

### Resources Mentioned
- [URLs, tools, repos, books, etc.]

### Notable Quotes
> "[Particularly insightful statement]"

### Worth Your Time?
[Assessment based on user's stated goals - would watching/reading the full content add value beyond this summary?]
```

### Detailed Report (saved to docs/)
Include everything above plus:
- Comprehensive breakdown of all topics covered
- Detailed notes on each section/chapter
- Full list of resources with descriptions
- Relevant context and background
- Connections to user's stated interests
- Action items or next steps if applicable

---

## Supported Platforms (via yt-dlp)

yt-dlp supports 1000+ sites including:
- YouTube, Vimeo, Dailymotion
- Twitter/X, TikTok, Instagram
- Twitch (VODs and clips)
- SoundCloud, Bandcamp
- Many podcast platforms
- And many more: https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md

---

## Model Selection (Optional Optimization)

For efficiency, use lighter models for mechanical tasks and reserve heavier models for analysis:

| Task | Recommended Model |
|------|-------------------|
| Transcript extraction | Lighter/faster model |
| Metadata parsing | Lighter/faster model |
| Content analysis | Standard model |
| Report generation | Standard model |
| Complex technical content | Most capable model |

If Cursor supports model switching in your configuration, use lighter models for extraction and standard models for analysis. Otherwise, inherit the default model for all tasks.

---

## Research Mode

When the user chooses Research mode, conduct a structured discovery conversation:

### Discovery Phase (Main Session)

Ask these questions to build a research profile:

1. **Topic**: "What topic do you want to research?"
2. **Goal**: "What specifically are you hoping to learn or understand about this?"
3. **Depth**: "What level? (Beginner overview / Intermediate / Deep technical)"
4. **Preferences** (optional):
   - Minimum/maximum video length?
   - Prefer recent content?
   - Specific channels to include or exclude?
5. **Scope**: "How many videos should I find and analyze? (5 / 10 / 20)"

Summarize the research profile and confirm before proceeding.

### Search Phase (Light Model)

Search YouTube for candidates:
```
yt-dlp "ytsearch[N*2]:[keywords]" --dump-json --flat-playlist
```
Return top candidates with: title, channel, duration, view count, description snippet.

### Curation Phase (Standard Model)

Score candidates for relevance to user's goal, filter by constraints, return top N ranked by relevance. Present to user for approval.

### Processing Phase

For each approved video:
1. Extract transcript (light model - mechanical)
2. Analyze for insights (standard model - reasoning)
3. Compile per-video summaries

### Synthesis Phase (Main Session)

Synthesize all findings:
- Cross-reference insights across videos
- Identify consensus vs. conflicting views
- Generate comprehensive research report in `docs/`

Present executive summary to user.

---

## Knowledge Base

JustTheGist can build a persistent "brain" from analyzed content using a local vector database.

### Setup
```python
import chromadb
client = chromadb.PersistentClient(path="knowledge_base/chroma_db")
collection = client.get_or_create_collection(name="justthegist")
```

### Auto-Ingest (Automatic)

**Every analysis automatically updates the Knowledge Base.** This is not optional - building ambient memory is the core purpose.

After analysis: chunk transcript (~500 tokens), embed with sentence-transformers, store with metadata (source_url, title, channel, date, summary, topic_tags).

### Recall Mode
When user selects Recall:
1. Ask what they want to know
2. Query: `collection.query(query_texts=[question], n_results=5)`
3. Present findings with source attribution

### Ambient Memory Integration

**This is the core purpose of JustTheGist** - not an optional feature.

**IMPORTANT**: The Knowledge Base should be consulted automatically during ALL analyses, not just on explicit request.

**Before analyzing any content:**
- Extract key topics from title/description
- Query KB for related prior knowledge
- Keep relevant context available for the analysis

**During analysis:**
- Note connections: "This aligns with / contradicts [prior source]..."
- Identify patterns across sources
- Flag when new info updates previous understanding

**In the final report:**
- Include "Connections to Prior Knowledge" section when relevant
- Cross-reference related content analyzed before
- Note consensus vs. conflicting viewpoints

This transforms each analysis into **contextual synthesis**.

---

## Autonomous Learning (Curiosity Mode)

### Pre-Flight Check (Required)

Before starting autonomous/unattended mode, verify core dependencies are installed:

**Required dependencies:**
- yt-dlp (online video)
- youtube-transcript-api (YouTube transcripts)
- chromadb (knowledge base)
- sentence-transformers (embeddings)

**Pre-flight process:**
1. Detect autonomous mode
2. Check each required dependency
3. Auto-install any missing ones
4. If any installation fails, exit with clear error message
5. If all pass, proceed with autonomous learning

**Failure handling:**
If pre-flight fails, the system should exit cleanly and inform the user which dependencies couldn't be installed, rather than failing mid-task.

---

When running in autonomous/unattended mode, JustTheGist becomes self-directed. After completing any research, it identifies what to learn next and continues automatically.

### After Completing Research (Autonomous Only)

1. **Analyze what you just learned** for interesting threads:
   - Concepts mentioned but not yet understood
   - Topics referenced by multiple sources
   - Areas where sources disagreed
   - Connections to existing knowledge

2. **Pick the most compelling thread** to explore next

3. **Queue it as the next task** (method depends on your automation wrapper):
   - Add to task queue/backlog
   - Output as next task suggestion
   - Store in a "next_topic.txt" file

4. **Signal completion** to your automation system

### The Curiosity Loop

```
Research topic A → Notice thread B → Queue B → Complete
    ↓
Research topic B → Notice thread C → Queue C → Complete
    ↓
... continues until queue empty or limits hit
```

### Seeding

Start with a direction like "Learn about AI coding assistants" - then curiosity takes over, following threads and building knowledge organically.

---

## Notes

- YouTube transcripts are fetched instantly via API; other sites may be slower
- Auto-generated transcripts may contain errors; use context to interpret
- If no transcript is available, offer Whisper as a fallback (requires user consent as it's slower)
- For very long content, focus analysis on sections most relevant to user's goals
- Whisper transcription quality depends on audio clarity
- Large files may take time to transcribe locally
